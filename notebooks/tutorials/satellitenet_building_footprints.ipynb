{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfabc700-d134-479a-af37-6d66fa609bb0",
   "metadata": {},
   "source": [
    "In this notebook we will have a look at the [SpaceNet 1: Building Detection](https://spacenet.ai/spacenet-buildings-dataset-v1/) challenge. The goal in this challenge is to find building footprints in satellite images. The focus in this notebook will be on the way that image labels can be transformed in order to make them usable in a more standardized way for classical image recognition models and on a way how to measure the success of the model.\n",
    "\n",
    "In order to use something like a neural network for image recognition, the model needs to be trained with labeled data. This labeled data usually needs to be in a standardized format. For example, if a model should do image classification, the input dimension of the model is usually strictly defined: it equals a fixed number of pixels of input images, like for example 224x224 pixel RGB images. And on the output side the data also needs to comply with a fixed dimension which usually coincides with the number of classes that should be predicted. A classification into cats and dogs would have an output of 2 numbers: the individual likelihoods for classes cats and dogs. In contrast to that, image segmentation models will assign each individual pixel of an image to one class out of a set of class labels. Hence, the output dimension will be equal to (or a multiple of) the number of image pixels. \n",
    "\n",
    "Although the SpaceNet challenge goal is pretty similar to a image segmentation task, the labels are not given as labeled image pixels, but as vector shapes with georeferenced coordinates. It would be a problem to somehow try to directly predict those vector shapes, as they do not comply with a standardized output dimension: any building footprint might have a different number of edges of its vector shape and any image might have a different number of building footprints in it. Hence, it is not straightforward how the data should be transformed into a more standardized setting such that it can be used to train a model on it. \n",
    "\n",
    "Additionally, one needs to think about a metric in order to measure whether the model did do a good job or not. In image classification tasks this is rather straightforward: for each image you can check whether the image class was predicted correctly or not. With multiple georeferenced building footprints you run into several problems, however, where it is not directly obvious how to measure success or failure. For example, any given building footprint might be partially detected. How much overlap would you then need between true and predicted building footprint in order to count this as a success? Also, two separate building footprints that are close together might be detected with just a single predicted overlapping building footprint. How well was the task fulfilled in this case?\n",
    "\n",
    "We will dive into all of these problems and implement one possible solution that was proposed by SpaceNet as a helpful starting point for the challenge. If you want further details on the topic, please also have a look at the following two blog posts that I used to come up with this notebook:\n",
    "\n",
    "- [Getting started with SpaceNet data](https://medium.com/the-downlinq/getting-started-with-spacenet-data-827fd2ec9f53)\n",
    "- [The SpaceNet Metric](https://medium.com/the-downlinq/the-spacenet-metric-612183cc2ddb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298b141-c777-42ae-9811-0d5ad6bacbdf",
   "metadata": {},
   "source": [
    "## Load and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d47e40-9e82-4d4c-8e16-52476839d5ad",
   "metadata": {},
   "source": [
    "Let's import some libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a246940-40a5-4351-aa81-11ce3213bd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import matplotlib \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show # import the show function which allows us to display the image\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "from rasterio import features\n",
    "from shapely import Point\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from matplotlib.patches import Circle\n",
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba10621-8866-4b54-98eb-0b2ae77420c7",
   "metadata": {},
   "source": [
    "Now we can load the `.tif` raster data with `rioxarray`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a343968-9e0c-4125-a663-90a6accead7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raster_file = '../../data/external/spacenet/3band_AOI_1_RIO_img5792.tif'\n",
    "geojson_file = '../../data/external/spacenet/Geo_AOI_1_RIO_img5792.geojson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c2ead-e510-43ee-8885-6c65fc66e357",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataarr = rioxarray.open_rasterio(raster_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdff0bb-6c16-49a3-ab0c-b5ee67680206",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99c3f88-ebd5-41c6-940c-62b8fa1bcd4d",
   "metadata": {},
   "source": [
    "The `xarray.DataArray` comes with geospatial metadata and can be plotted easily with georeferenced axis ticks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b02961-4896-449c-be67-0681dffdb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataarr.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b95eba-4f61-48a4-b54e-6bfbc4af6fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataarr.plot.imshow()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940e9c7-1df7-4001-8a08-50eeb179927f",
   "metadata": {},
   "source": [
    "Alternatively, we can also drop the geo-spatial location information and plot just the image from a `numpy` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab43d9-6f31-446a-a9b8-e17b6dc31065",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = img_dataarr.values\n",
    "img_data = np.transpose(img_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f84dda-20ed-45a6-8722-bfdc53a86f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#img = np.dstack((red_band, green_band, blue_band))\n",
    "f = plt.figure()\n",
    "plt.imshow(img_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d3d9ad-2657-466d-bbde-b437f21af9ec",
   "metadata": {},
   "source": [
    "Now let's read in the data labels using GeoPandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97cf355-b678-482c-bdab-2de005867ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_footprints = gpd.read_file(geojson_file)\n",
    "building_footprints.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936be989-fb0d-4634-95c5-a38e428dc432",
   "metadata": {},
   "source": [
    "GeoPandas automatically used the building footprints as geometry of the DataFrame. The individual polygons can be easily visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32bd416-ec97-4349-a7a2-0d36e4a660c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_footprints.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c13ae-5993-45b1-98f6-2fe02e44b119",
   "metadata": {},
   "source": [
    "Putting raster data and image labels into a single chart we can check how well the data was labeled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba132120-fa8c-4de5-bdd4-ac3dfeeed576",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax0 = plt.subplots(1, 1, figsize=(8,8))\n",
    "\n",
    "img_dataarr.plot.imshow(ax=ax0)\n",
    "building_footprints.plot(ax=ax0, alpha=0.25, edgecolor='None', facecolor='orange')\n",
    "building_footprints.plot(ax=ax0, alpha=0.75, edgecolor='red', facecolor='None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86544b3f-35dd-4830-bcfb-08f489ff1f3a",
   "metadata": {},
   "source": [
    "As you can see, building footprint rectangles are sometimes off by some pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb70c53-6bca-4f14-ba55-632d02207146",
   "metadata": {},
   "source": [
    "## Rasterize vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19bd56c-fd73-40d4-becb-5efae2458398",
   "metadata": {},
   "source": [
    "Now the next step will be to get the labels into a more standardized format. For this, we will \"rasterize\" the vector data and \"burn\" the vector shapes into a raster file. The output will be a simple `numpy` array without geospatial information. In this process the `rasterize` function needs to know the transformation that is required to get from image pixel coordinates to georeferenced coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c24502-17eb-4af1-84e6-dee395111706",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataarr.rio.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1014a19-9238-4ca5-91f4-57fa77d4cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize vector using the shape and coordinate system of the raster\n",
    "rasterized = features.rasterize(building_footprints.geometry.values,\n",
    "                                out_shape = img_dataarr.rio.shape,\n",
    "                                fill = 0,\n",
    "                                out = None,\n",
    "                                transform = img_dataarr.rio.transform(),\n",
    "                                all_touched = False,\n",
    "                                default_value = 1,\n",
    "                                dtype = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40644560-01a0-4c61-87ba-69cd29dc64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rasterized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9979f327-0788-408e-b04a-3c84a184b4d9",
   "metadata": {},
   "source": [
    "We now have a nice pixel / raster representation of the original vector data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d208877-22e7-48ea-8c8c-a54846da998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raster\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "\n",
    "img_dataarr.plot.imshow(ax=ax0)\n",
    "building_footprints.plot(ax=ax0, alpha=0.25, edgecolor='None', facecolor='orange')\n",
    "building_footprints.plot(ax=ax0, alpha=0.75, edgecolor='red', facecolor='None')\n",
    "\n",
    "show(rasterized, ax=ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1b95f5-a408-4d49-b18e-710230cb6ea8",
   "metadata": {},
   "source": [
    "## Verify x/y indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e9bcd-93e7-4273-985a-dd64dcd86004",
   "metadata": {},
   "source": [
    "At this point it might be helpful to have a closer look at the way that the data is indexed. The raster matrix is transposed in a sense that the index with label `x` is stored in columns. This can be seen from the shape of the index. `x` has 439 entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fe84b-58de-45ea-8ae6-fd41107a90de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataarr.indexes.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38d2ec-ee26-4376-8035-cad00871cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataarr.x.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261a027-9702-4026-9dc1-dab2c8b08549",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataarr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1650d397-12f1-434d-810b-6d82ae5f5d5c",
   "metadata": {},
   "source": [
    "And these 439 entries are stored as second dimension (columns) in the `numpy` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fc353-817e-4b60-a33c-5f2798b93e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953b1ce-3617-4938-8438-ada0285d7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rasterized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a69b1-2c78-45ee-bd98-b5cb75c8b9c2",
   "metadata": {},
   "source": [
    "In other words, in the `numpy` array the first dimension is `y`, while the second dimension is `x`. This is different for shapely geometries where the first dimension relates to `x` coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d04c7-ee8b-44c4-bd47-298bea14f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_poly = building_footprints.geometry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34d9cf5-fe43-4a24-98a1-f4c590f8d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_poly.exterior.coords.xy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371c1e4-e4f6-49da-8563-c03d8f793c23",
   "metadata": {},
   "source": [
    "Let's look at an example to verify this. We pick some arbitrary point in pixel space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080acb92-f6e3-43bf-9d1e-6d6e3ba4bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "irow = 283 # rows, represents y coordinates\n",
    "icol = 113 # columns, represents x coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84fd0ac-5ab7-4d50-b260-ec009e50ff31",
   "metadata": {},
   "source": [
    "Now in order to overlay this point into the pixel image we will create a `Circle` object. For a circle, the input coordinates need to be given as (x,y) tuple. For this we need to keep in mind that `x` is represented as column in the raster image. Hence, the tuple needs to be (icol, irow) now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84df74-75ce-435d-854f-4431149cb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_circle = Circle((icol, irow), radius=5, color='red') # Circle coordinates are given as (x,y); irow represents y, icol represents x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dc7264-b95a-4dfc-9059-24412a780ceb",
   "metadata": {},
   "source": [
    "Also, we'll manipulate some image values and overwrite them with a white rectangle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a8fbd-8e2a-4206-811d-fd31491ccdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = img_dataarr.values.copy()\n",
    "img_data = np.transpose(img_data, (1, 2, 0))\n",
    "\n",
    "# make rectangular shape with color\n",
    "this_fake_val = 250\n",
    "img_data[0:10, 0:50, 0] = this_fake_val\n",
    "img_data[0:10, 0:50, 1] = this_fake_val\n",
    "img_data[0:10, 0:50, 2] = this_fake_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d050171-a0c2-4f1f-acec-3d861663bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be316c9-4768-4a7f-9173-52b98b3310c5",
   "metadata": {},
   "source": [
    "Now if we visualize the data we can see that the rectangle's larger side extends into dimension `x`, although we manipulated more columns than rows in the `numpy` array. This might go against the intuition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a852a94-c385-4209-8c14-6b176ec1a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img_data)\n",
    "ax.add_patch(this_circle)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96499a-9340-41b8-ab17-03a4566acda9",
   "metadata": {},
   "source": [
    "At this point, let us also have a closer look at the transformation between pixel space and geo-spatial coordinates. In order to get from pixel space to coordiantes we can easily use the transformation that is stored in the `xarray`. For example, we can map the top left pixel (0,0) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b07ccf-7cc2-4b9d-b1ab-e60ff25ea0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_coord, y_coord) = rasterio.transform.xy(img_dataarr.rio.transform(), 0, 0)\n",
    "x_coord, y_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6c56f9-23bc-44cc-a46b-aa9da66eea54",
   "metadata": {},
   "source": [
    "Let's verify this by plotting a red circle at the transformed coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3362bc-9cba-415f-8c34-2b1d9d5440c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "img_dataarr.plot.imshow(ax=ax)\n",
    "ax.add_patch(Circle((x_coord, y_coord), radius=0.0001, color='red'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c0607f-2200-4757-a984-5dfe08c61949",
   "metadata": {},
   "source": [
    "The same formula also holds for an arbitrary point of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fd515-a77e-4589-bb1d-2ed09869ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coord, y_coord = img_dataarr.rio.transform() * (icol, irow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f7ee1-1649-4349-a0ff-0800ec86e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_coord, y_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2f363-cb4c-4f46-9ee1-f6f0db3a129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative code would be:\n",
    "# (y_coord, x_coord) = rasterio.transform.xy(img_dataarr.rio.transform(), irow, icol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244df822-e5eb-42e9-8ca8-58f6c2295db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "img_dataarr.plot.imshow(ax=ax)\n",
    "ax.add_patch(Circle((x_coord, y_coord), radius=0.00002, color='red'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906042c9-c3e9-4e4a-b200-736196e622f4",
   "metadata": {},
   "source": [
    "## Measure distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1374cc24-60cb-45ae-9c4b-1c9226b9bdbd",
   "metadata": {},
   "source": [
    "Now we want to start thinking about how to measure distances between pixels and geometries. We will use this to find an overlapping geometry for the point that we have chosen. Let's first pick some arbitrary polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347354e1-368f-44a9-b740-ca56d6cad0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_poly = building_footprints.geometry[0]\n",
    "this_poly_gdp = gpd.GeoDataFrame(geometry=[this_poly], crs=img_dataarr.rio.crs)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "img_dataarr.plot.imshow(ax=ax)\n",
    "\n",
    "ax.add_patch(Circle((x_coord, y_coord), radius=0.00002, color='red'))\n",
    "this_poly_gdp.plot(ax=ax, facecolor='None', edgecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f3584c-758f-4b47-890a-2c15d00c00bd",
   "metadata": {},
   "source": [
    "We can see that the x/y dimensions of the point is in line with the x/y dimensions from the polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e03fac-5ab7-4c98-adc9-78bbdf4fc1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_poly.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf94230-d274-4d7a-9f0e-cd7668ea0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_point = Point(x_coord, y_coord)\n",
    "this_point.coords[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66546d35-70d3-4b24-91eb-8749db1cedb7",
   "metadata": {},
   "source": [
    "Hence, the distance between our point and the polygon can be easily computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8922b1-7a79-42e2-85ab-dd1efca4189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newpd = this_point.distance(this_poly.boundary)\n",
    "newpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4fd33c-f03e-4fad-a661-21fd995ef277",
   "metadata": {},
   "source": [
    "In order to find the closest polygon to our point we simply need to loop over all polygons and keep track of the minimum distance or any polygon that fully contains the point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112c0cd-e7a5-4ba3-bbcf-089d4403b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "min_counter = 0\n",
    "running_min_dist = 100000\n",
    "poly_contain_index = None\n",
    "\n",
    "for this_poly in building_footprints.geometry:\n",
    "    \n",
    "    this_dist = this_point.distance(this_poly.boundary)\n",
    "    if this_dist < running_min_dist:\n",
    "        running_min_dist = this_dist\n",
    "        min_counter = counter\n",
    "    \n",
    "    if this_poly.contains(this_point):\n",
    "        poly_contain_index = counter\n",
    "        \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6515db-ee41-4811-8f37-3462a96d99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdb07c-270a-4ba4-b27a-0279bebaf123",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_contain_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe625330-0ddc-45aa-98d8-1f5be45f1e25",
   "metadata": {},
   "source": [
    "Let's verify that we picked the correct polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870eaaa9-8f02-4476-a8c8-3ce97d6d616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_poly = building_footprints.geometry[poly_contain_index]\n",
    "this_poly_gdp = gpd.GeoDataFrame(geometry=[this_poly], crs=img_dataarr.rio.crs)\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "img_dataarr.plot.imshow(ax=ax)\n",
    "\n",
    "ax.add_patch(Circle((x_coord, y_coord), radius=0.00002, color='red'))\n",
    "this_poly_gdp.plot(ax=ax, facecolor='None', edgecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d3a19-ce9a-4d44-8949-1c077c3c0281",
   "metadata": {},
   "source": [
    "## Distance transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d827355-3be6-442e-abb8-1310cb458181",
   "metadata": {},
   "source": [
    "We've already seen one way of how the building footprint vector labels can be transformed into a standardized format by rasterizing the vector data. However, this label transformation comes with one potential problem: building footprints that are too close to each other will end up as one connected segment and hence the model can not be able to distinguish the individual buildings anymore.\n",
    "\n",
    "There is an alternative approach that might help to better preserve individual buildings. The idea is to compute for any pixel the distance to the closest building bounds and use this distance to encode building information. For pixels that are inside of a building we encode this distance with a negative sign such that we can better distinguish interior and exterior points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817ed0e-8547-4d80-9027-e4ba23408ca7",
   "metadata": {},
   "source": [
    "A straightforward implementation of this approach is to loop over all polygons for each pixel and compute the distance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398b144-b864-4599-acfb-c8d07788ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_point = Point(x_coord, y_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd962b3-f0c5-49d7-a3b3-d66429e2e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_distance(this_point, building_footprints):\n",
    "    \n",
    "    counter = 0\n",
    "    running_min_dist = 100000\n",
    "    \n",
    "    for this_poly in building_footprints.geometry:\n",
    "        \n",
    "        this_dist = this_point.distance(this_poly.boundary)\n",
    "        \n",
    "        if this_poly.contains(this_point):\n",
    "            \n",
    "            this_dist = -1 * this_dist\n",
    "            running_min_dist = this_dist\n",
    "            \n",
    "            return running_min_dist\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if this_dist < running_min_dist:\n",
    "            \n",
    "                running_min_dist = this_dist\n",
    "\n",
    "    return running_min_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b3bcb7-f165-4503-bfd8-de99ff2ee678",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows, n_cols, _ = img_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8a5eb-7998-4d89-b46e-03cf085301dc",
   "metadata": {},
   "source": [
    "However, this implementation would be very slow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e241b-d882-4f72-897e-b941669c1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "min_dist_matr = np.zeros((n_rows, n_cols))\n",
    "\n",
    "for icol in range(0, n_cols):\n",
    "    for irow in range(0, n_rows):\n",
    "        x_coord, y_coord = img_dataarr.rio.transform() * (icol, irow)\n",
    "        this_point = Point(x_coord, y_coord)\n",
    "        \n",
    "        min_dist = get_min_distance(this_point, building_footprints)\n",
    "        min_dist_matr[irow, icol] = min_dist\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f351f0c-f101-400d-95d0-5ad6085fb3e8",
   "metadata": {},
   "source": [
    "This is how the building information looks like when rasterized using the distance transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de1f64-1379-4a61-90a2-9f03564e1e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "\n",
    "ax0.imshow(min_dist_matr, vmin=-0.00001, vmax=0.0001, cmap='jet_r')\n",
    "#ax0.colorbar()\n",
    "\n",
    "show(rasterized, ax=ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f05e0e-81d5-4534-9328-c6ac7dadc6b3",
   "metadata": {},
   "source": [
    "We can get a very similar approach also by using the `distance_transform_edt` function from the `scipy` image transformation tools. We need to apply it twice in order to get distances for both interior and exterior points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b4d4d-b3e8-4279-976e-bdca95703395",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "im_dist = distance_transform_edt(rasterized)\n",
    "\n",
    "rasterized_inv = 1 - rasterized\n",
    "im_dist_inv = distance_transform_edt(rasterized_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d74000-d77f-4e9e-b668-042aa754622d",
   "metadata": {},
   "source": [
    "Let's merge interior and exterior point distances into a single rasterized image with negative distances for interior values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c7e79-d1aa-45ae-8d29-64ab38804311",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dist_merged = (-1)*im_dist.copy()\n",
    "im_dist_merged[im_dist_inv > 0] = im_dist_inv[im_dist_inv > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9faf56e-9236-46aa-8a79-f3173b578475",
   "metadata": {},
   "source": [
    "For the visualization we need to deal with the fact that distances now live on a different scale than in the approach before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214f1bf-10df-4d8c-8d05-a4c946f3695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(im_dist_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7ba69-9295-4fbb-84ea-90af5d7ba27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(im_dist_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0d98a4-7006-44d5-acee-57a4580057e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(min_dist_matr), np.max(min_dist_matr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c4121-2a46-4293-8089-38c9dfb049f9",
   "metadata": {},
   "source": [
    "But with some reasonable settings for the color map we get an output that resembles the output from the slow implementation pretty closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972908c4-c14f-4b08-98d6-49e04219762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "\n",
    "ax0.imshow(min_dist_matr, vmin=-0.00001, vmax=0.0001, cmap='jet_r')\n",
    "#ax0.colorbar()\n",
    "\n",
    "plt.imshow(im_dist_merged, vmin=-7, vmax=10, cmap='jet_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f05013-eb0e-4de5-9210-90f8d74160aa",
   "metadata": {},
   "source": [
    "## Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d51344-25ec-4e1b-a806-45b4d34e32f3",
   "metadata": {},
   "source": [
    "The original image labels consisted of vector data building footprints. When we transform those labels into rasterized proxies and then use them to train a model then we actually solve a proxy problem instead of a real problem. In the end, we will need to transform the proxy output back to the true data format that we want to have: vector data.\n",
    "\n",
    "In order to do this, we will use a clustering algorithm similar to the one described here: https://gist.github.com/hagerty/724b84ad69897d1fe6d241bbca9e2781. The idea is pretty simple. Let's say we now denote all interior values of buildings with positive values. Then within any building there should be a maximum value somewhere at the center of the building. And from there the distance values decrease on paths towards the building borders. Hence, starting from the maximum value we can grow a cluster by extending the cluster with neighboring points whenever they have decreasing value and simultaneously also positive values that denote the interior of buildings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f298acc1-0e11-4552-9313-1622fd7ab1bb",
   "metadata": {},
   "source": [
    "Let's first create an image with positive values for building interiors and zeros elsewhere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3f106a-7f76-45f0-983c-299af50da2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_values = (min_dist_matr > 0)*(1)\n",
    "plt.imshow(pos_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e258767-647d-474e-acb1-3e17874d3cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_estimates = (-1)*im_dist_merged.copy()\n",
    "building_estimates[building_estimates < 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89d353f-0f25-4b8f-81e1-647393cbbf2f",
   "metadata": {},
   "source": [
    "Now we will apply the cluster growing algorithm to the image. We keep track of all building pixels that are not yet added to some cluster in a variable `to_be_classified`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab3b1fc-9f5f-4e42-b73f-189bd73e0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_classified = building_estimates > 0 # all positive values are inside of a building\n",
    "cluster_int = 1\n",
    "all_clusters = building_estimates * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac7e3b-c5fd-49f6-8be8-08d80b173c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(to_be_classified)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e592e1-9de0-4125-b4ca-c163e41df559",
   "metadata": {},
   "source": [
    "The algorithm starts with some cluster and grows this cluster until no neighoring points can be added anymore. Then the cluster points are removed from the `to_be_classified` matrix and it will start with the next cluster until all points in `to_be_classified` are classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e9ec1-704d-4d93-a061-16eb30a1d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_remaining_maximum(building_estimates, to_be_classified):\n",
    "    \"\"\"\n",
    "    Find maximum value of remaining pixels. Positive values denote building interiors.\n",
    "    \"\"\"\n",
    "    \n",
    "    peak = np.argmax(np.multiply(building_estimates, to_be_classified))\n",
    "    \n",
    "    i_row = int(peak/building_estimates.shape[1])\n",
    "    i_col = int(peak - i_row * building_estimates.shape[1])\n",
    "    \n",
    "    return i_row, i_col\n",
    "\n",
    "def decreasing_neighbor_check(shift_matr, matr):\n",
    "    \"\"\"\n",
    "    For a left-, right-, top- or bottom-shifted matrix, check whether new cluster neighbors are of decreasing slope\n",
    "    \"\"\"\n",
    "\n",
    "    slope_matr = shift_matr - matr\n",
    "    decreasing_neighbors = np.logical_and((slope_matr > 0), shift_matr > 0)#.astype(int)\n",
    "\n",
    "    return decreasing_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b34d67-6bc8-4565-91a2-6e52608fcb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "while np.any(to_be_classified):\n",
    "    \n",
    "    if cluster_int > 100:\n",
    "        print('Emergency break')\n",
    "        break\n",
    "        \n",
    "    i_row, i_col = get_remaining_maximum(building_estimates, to_be_classified)\n",
    "    \n",
    "    # initialize current cluster\n",
    "    this_cluster = building_estimates * 0\n",
    "    this_cluster[i_row, i_col] = 1\n",
    "    to_be_classified[i_row, i_col] = False\n",
    "\n",
    "    cluster_not_final = True\n",
    "    \n",
    "    # start of WHILE loop\n",
    "\n",
    "    while cluster_not_final:\n",
    "\n",
    "        cluster_size = np.sum(this_cluster > 0)\n",
    "        #print(f'Current cluster size: {cluster_size}')\n",
    "\n",
    "        # grow current cluster with neighboring pixels (that are still to be classified) of decreasing values\n",
    "        matr = this_cluster.copy()\n",
    "\n",
    "        # create shifted matrices\n",
    "        n_rows, n_cols = matr.shape\n",
    "        col_zeros = np.zeros((n_rows, 1))\n",
    "        row_zeros = np.zeros((1, n_cols))\n",
    "\n",
    "        right_shift = np.hstack((col_zeros, matr[:, 0:-1]))\n",
    "        left_shift = np.hstack((matr[:, 1:], col_zeros))\n",
    "        top_shift = np.vstack((row_zeros, matr[0:-1, :]))\n",
    "        down_shift = np.vstack((matr[1:, :], row_zeros))\n",
    "\n",
    "        # check for decreasing neighbors\n",
    "        candidates_right = decreasing_neighbor_check(right_shift, matr)\n",
    "        candidates_left = decreasing_neighbor_check(left_shift, matr)\n",
    "        candidates_top = decreasing_neighbor_check(top_shift, matr)\n",
    "        candidates_down = decreasing_neighbor_check(down_shift, matr)\n",
    "\n",
    "        # aggregate\n",
    "        new_cluster_points = (candidates_right | candidates_left | candidates_top | candidates_down) & to_be_classified\n",
    "\n",
    "        n_new_points = np.sum(new_cluster_points > 0)\n",
    "\n",
    "        # update cluster and to_be_classified\n",
    "        this_cluster[new_cluster_points] = cluster_int\n",
    "        to_be_classified[new_cluster_points] = False\n",
    "\n",
    "        if n_new_points == 0:\n",
    "            \n",
    "            print(f'New cluster done: {cluster_int} with {np.sum(this_cluster > 0)} points')\n",
    "            cluster_not_final = False\n",
    "            all_clusters[this_cluster > 0] = cluster_int\n",
    "\n",
    "#             fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "\n",
    "#             # visualize current cluster\n",
    "#             ax0.imshow(this_cluster)\n",
    "#             ax1.imshow(to_be_classified)\n",
    "            \n",
    "#             plt.show()\n",
    "            \n",
    "            cluster_int += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6a9b6-ee31-40e3-b742-f618983b4e7f",
   "metadata": {},
   "source": [
    "Let's visualize how well this post-processing algorithm is suited to transform the rasterized labels back to separate buildings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2738953-f51a-4804-925c-5a500ba21b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clusters_disp = all_clusters.copy()\n",
    "all_clusters_disp[all_clusters == 0] = -200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9a3e7-45ee-4a5b-a408-fda2e3f094a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raster\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "\n",
    "img_dataarr.plot.imshow(ax=ax0)\n",
    "building_footprints.plot(ax=ax0, alpha=0.25, edgecolor='None', facecolor='orange')\n",
    "building_footprints.plot(ax=ax0, alpha=0.75, edgecolor='red', facecolor='None')\n",
    "\n",
    "ax1.imshow(all_clusters_disp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f0ef1e-6d5f-41ae-8b9d-07dcb40d0d1a",
   "metadata": {},
   "source": [
    "As we can see, the individual clusters seem to match the true vector footprints pretty well. However, the two adjacent buildings in the upper right corner will erroneously end up in a single cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de054d-e0e7-4f37-8fdf-9357d429a2c7",
   "metadata": {},
   "source": [
    "### Transform to vector geometries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993d2223-1230-412e-b26c-f059b01d4ef1",
   "metadata": {},
   "source": [
    "Now we only lack one final step: we need to translate the pixel clusters to vector polygons with correct georeferenced coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59b57e-0438-4abd-90de-6d415568d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = int(np.max(all_clusters)) # implicit assumption of consecutively numbered clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c550c-d02b-4d58-81b5-87cf64feb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_polys = []\n",
    "\n",
    "for this_cluster_id in range(1, n_clusters+1):\n",
    "    \n",
    "    this_mask = all_clusters == this_cluster_id\n",
    "    \n",
    "    # vectorize polygon\n",
    "    results = ({'properties': {'raster_val': v}, 'geometry': s}\n",
    "            for i, (s, v) \n",
    "            in enumerate(\n",
    "                shapes(all_clusters.astype(rasterio.int16), mask=this_mask, transform=img_dataarr.rio.transform())))\n",
    "    \n",
    "    # get potentially multiple polygons\n",
    "    geoms = list(results)\n",
    "    assert len(geoms) == 1, f'Multiple polygons found for cluster {this_cluster_id}'\n",
    "    \n",
    "    this_poly = shape(geoms[0]['geometry'])\n",
    "    estimated_polys.append(this_poly)\n",
    "    \n",
    "estimated_polys_gpd = gpd.GeoDataFrame(estimated_polys, geometry=estimated_polys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530af35f-1077-40f9-b8e9-c84480826fcb",
   "metadata": {},
   "source": [
    "As we can see, the vectorized polygons match the original labels pretty closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0655dd-eded-4a04-878e-ecf9c4700da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raster\n",
    "fig, (ax0, ax1) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "\n",
    "img_dataarr.plot.imshow(ax=ax0)\n",
    "building_footprints.plot(ax=ax0, alpha=0.25, edgecolor='None', facecolor='orange')\n",
    "building_footprints.plot(ax=ax0, alpha=0.75, edgecolor='red', facecolor='None')\n",
    "\n",
    "estimated_polys_gpd.plot(ax=ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2447b2e6-f8b8-4865-abab-a75867647c9d",
   "metadata": {},
   "source": [
    "This can best be seen when we plot both in a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19421e2-1e2e-4aa8-9d19-ef37d1af6e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raster\n",
    "fig, ax0 = plt.subplots(1, figsize = (6, 6))\n",
    "\n",
    "\n",
    "#building_footprints.plot(ax=ax0, alpha=0.25, edgecolor='None', facecolor='orange')\n",
    "estimated_polys_gpd.plot(ax=ax0)\n",
    "building_footprints.plot(ax=ax0, alpha=0.75, edgecolor='red', facecolor='None')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20f2ca3-287a-414d-8f43-d1c1adb25d67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compute score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa2c512-ed33-4d03-a286-a913f26ef611",
   "metadata": {},
   "source": [
    "Now that we already have an algorithm to translate pixel labels into georeferenced polygons, we still need to think about a way how we can evaluate and measure how well any post-processed polygons match their original labels. One metric proposed by SpaceNet is the Intersection-over-Union (IoU) measure:\n",
    "\n",
    "$$IoU(A,B) = \\frac{A\\cap B}{A \\cup B}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfa8c7-f228-4f98-a813-16ca7217fc7c",
   "metadata": {},
   "source": [
    "We compute all pairwise IoUs and keep all IoUs above a threshold of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a61f1-5e60-4142-90eb-700f28726c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_polys = building_footprints.geometry\n",
    "estimated_polys = estimated_polys_gpd.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d71033-026f-41eb-9c5a-fcbbfe7f47f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_true_polys = len(true_polys)\n",
    "n_estimated_polys = len(estimated_polys)\n",
    "\n",
    "iou_thres = 0.5\n",
    "\n",
    "all_ious = np.zeros((n_true_polys, n_estimated_polys))\n",
    "\n",
    "for ii in range(0, n_true_polys):\n",
    "    for jj in range(0, n_estimated_polys):\n",
    "        \n",
    "        this_true_poly = true_polys[ii]\n",
    "        this_estimated_poly = estimated_polys[jj]\n",
    "\n",
    "        intersect_poly = this_true_poly.intersection(this_estimated_poly)\n",
    "        union_poly = this_true_poly.union(this_estimated_poly)\n",
    "\n",
    "        iou = intersect_poly.area / union_poly.area\n",
    "        \n",
    "        if iou >= iou_thres:\n",
    "            all_ious[ii, jj] = iou\n",
    "        else:\n",
    "            all_ious[ii, jj] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea354a9b-e0be-4ae9-a6b8-3f67c9616ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(all_ious)\n",
    "plt.title('Pairwise IoU values')\n",
    "plt.xlabel('Index estimated polygon')\n",
    "plt.ylabel('Index true polygon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ddb68a-fc1f-4959-ac2a-1dce87d40d26",
   "metadata": {},
   "source": [
    "Now we can create matches between true and estimated polygons. We do this by consecutively matching polyons with highest IoU value and eliminating any matched polygons from the list until no polygons are left anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095688ca-cfd5-457e-80b1-1f2e84bebbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_argmax_indices(matr):\n",
    "\n",
    "    peak = np.argmax(matr)\n",
    "    \n",
    "    i_row = int(peak/matr.shape[1])\n",
    "    i_col = int(peak - i_row * matr.shape[1])\n",
    "    \n",
    "    return i_row, i_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c1cfa6-d31b-480f-adea-490209b3782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_list_i = []\n",
    "est_list_i = []\n",
    "iou_list = []\n",
    "\n",
    "running_all_ious = all_ious.copy()\n",
    "counter = 0\n",
    "\n",
    "while np.any(running_all_ious) > 0:\n",
    "    if counter > 50:\n",
    "        print('Emergency break')\n",
    "        break\n",
    "    \n",
    "    i_true, i_est = get_argmax_indices(running_all_ious)\n",
    "    \n",
    "    true_list_i.append(i_true)\n",
    "    est_list_i.append(i_est)\n",
    "    iou_list.append(running_all_ious[i_true, i_est])\n",
    "    \n",
    "    running_all_ious[i_true, :] = 0\n",
    "    running_all_ious[:, i_est] = 0\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ea4eae-9491-4c77-b39d-d421b1eaaa8e",
   "metadata": {},
   "source": [
    "Let's keep track of all matches in a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22abd780-002c-4bf4-a2a3-05951e21db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched = pd.DataFrame(true_list_i, columns=['true_polygon_i'])\n",
    "df_matched['est_polygon_i'] = est_list_i\n",
    "df_matched['iou'] = iou_list\n",
    "df_matched.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc56737-5f78-4a8e-83e3-0711dd66164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1408cb8b-1c62-480e-8e44-aa906829de68",
   "metadata": {},
   "source": [
    "The algorithm can also leave some polygons unmatched. We will add them to the `DataFrame` as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66d1b5-8e8c-40e3-8a01-8e9b57dba4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimated_polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fb142-dd54-44b9-990a-c93fa1f1fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_true_inds = ~pd.Series(range(0, n_true_polys)).isin(df_matched['true_polygon_i'])\n",
    "unmatched_true_inds = unmatched_true_inds[unmatched_true_inds].index.values\n",
    "n_unmatched = len(unmatched_true_inds)\n",
    "\n",
    "df_unmatched = pd.DataFrame(unmatched_true_inds, columns=['true_polygon_i'])\n",
    "df_unmatched['est_polygon_i'] = np.nan\n",
    "df_unmatched['iou'] = np.nan\n",
    "df_unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c61a8-5698-491b-9b30-842086dc0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_est_inds = ~pd.Series(range(0, n_estimated_polys)).isin(df_matched['est_polygon_i'])\n",
    "unmatched_est_inds = unmatched_est_inds[unmatched_est_inds].index.values\n",
    "n_unmatched = len(unmatched_est_inds)\n",
    "\n",
    "df_unmatched_est = pd.DataFrame(unmatched_est_inds, columns=['est_polygon_i'])\n",
    "df_unmatched_est['true_polygon_i'] = np.nan\n",
    "df_unmatched_est['iou'] = np.nan\n",
    "df_unmatched_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00647096-6869-4342-9ae8-5c1a3f4fd367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched_unmatched = pd.concat([df_matched, df_unmatched, df_unmatched_est])\n",
    "df_matched_unmatched.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f434d-6487-4964-a195-6894b8fe5318",
   "metadata": {},
   "source": [
    "Let's inspect one case with high and low IoU match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6272ec7-0e76-4d97-880d-33d5604d2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_true_poly = building_footprints.geometry[df_matched.tail(1)['true_polygon_i'].squeeze()]\n",
    "this_estimated_poly = estimated_polys.geometry[df_matched.tail(1)['est_polygon_i'].squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58522fdc-4e1a-4b18-8477-970dd3f26d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gpd.GeoDataFrame([this_true_poly], geometry=[this_true_poly]).plot(facecolor='None', edgecolor='red')\n",
    "gpd.GeoDataFrame([this_estimated_poly], geometry=[this_estimated_poly]).plot(ax=ax, facecolor='None', edgecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f27726b-da81-465a-b140-3a4372712aed",
   "metadata": {},
   "source": [
    "For the best matching case the only deviation seems to come from the fact that the vectorized polygon still reflects the original raster pixel pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a205eace-cbbb-42e1-85cb-0f409486eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_true_poly = building_footprints.geometry[df_matched.head(1)['true_polygon_i'].squeeze()]\n",
    "this_estimated_poly = estimated_polys.geometry[df_matched.head(1)['est_polygon_i'].squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1199d5-ea18-4536-b145-738b5e9f0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gpd.GeoDataFrame([this_true_poly], geometry=[this_true_poly]).plot(facecolor='None', edgecolor='red')\n",
    "gpd.GeoDataFrame([this_estimated_poly], geometry=[this_estimated_poly]).plot(ax=ax, facecolor='None', edgecolor='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c0c6f4-a042-4b39-8649-10c044a3ce3c",
   "metadata": {},
   "source": [
    "Based on this table of matched/unmatched polygons we can now compute metrics like precision, recall and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30ef7d-3214-42aa-822f-cda821ce507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_matched_polys = df_matched.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2273f3-9498-48a3-9b66-bcf91b18bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = n_matched_polys / n_estimated_polys\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74981cc7-6f77-4f7f-b078-65e2c1e044ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = n_matched_polys / n_true_polys\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733414e-7529-4a79-9cae-4895929c6802",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score = 2*precision*recall / (precision+recall)\n",
    "f1_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
